### **Data Quality Metrics Comparison**

INTRODUCTION

I am interested in intra-tumor genetic heterogeneity and how the heterogeneity may play a role in cancer metastasis. For my first project in Lawson Lab, I worked with a single cell DNA whole exome sequencing (scWES) dataset that was generated by a labmate several years ago. My goal was to process the data and cluster the mutations to understand the mutational heterogeneity in the sample. My results were inconclusive, but I did not know why. The most likely reasons for inconclusive results are that 1) the data is of poor quality or 2) my analysis is wrong. To understand which of the two reasons is producing poor results, I chose to analyze a published dataset and compare various data quality metrics of both datasets.  In my project for this course, I compared four sequence data quality metrics between a published single-cell DNA-seq dataset (NCBI accession: SRA053195) and one generated in my lab. The comparison data was published by Wang et al and then again by Zafar et al. I chose this dataset because it uses a similar biological source as our lab’s, and the lab which generated the sequences -- Navin lab -- is a leader in single-cell DNA sequencing. 

METHODS

Before being able to do a comparative analysis, I first needed to download the published fastqs and then align them to the human reference genome with the same pipeline as I had used for processing the lab data, including some tertiary packages to get the quality metrics files. To download the fastqs, I used ```fastq-dump``` which is part of ```sratoolkit```. Then I aligned with ```bwa mem```. Alignment took the longest to modify and troubleshoot because the existing lab pipeline is for paired-end data, and the SRA data is single-end. Additionally, I used a package called ```bioinfokit```, which outputs a file with sequence depths in the same order as the input FASTQs, then GATK’s ```DiagnoseTargets``` on aligned BAMs to get a breadths file, and ```samtools stats``` to get duplicate reads data and Phred qualities. 

In this project I wrote bash and R [scripts](https://github.com/TatyanaLev/EE282/tree/finalProjectReport/code/scripts) to process the resulting files to extract the numbers I needed for comparison and to visualize the results. I used ```gawk``` a lot to select particular columns, ```grep``` to match regular expressions and pull out only particular lines, stream direction to a file, and piping. I used R to visualize data using boxplots and to run a statistical test. In R, I used ```cbind``` to merge columns and dataframe subsetting to select particular columns, as learned in class.  

Initially, I planned to analyze the following five QC metrics.

* average insert size -- library quality
* duplicate reads rate -- library QC and sequence loading optimization
* average Phred quality scores -- sequencing quality
* sequence depth -- sequencing loading optimization
* 5X sequence breadth -- library quality

Lastly, I subsetted our lab data from 95 cells to 32 so the number of cells in the two comparison groups can be the same for statistical analysis. I chose 32 cells with best breadth, as breadth is ultimately the most important metric in data analysis and determines which cells pass QC and can be clustered. I rationed that if our best cells are not as good as the published cells, then our average cells definitely aren’t good enough. To determine statistical significance I used Mann-Whitney U-test (Wilcoxon rank sum test) because my data is unpaired and I am not assuming normality in the distribution. R was used for the Wilcoxon test.

RESULTS

All boxplots (linked) show a difference in the data between our lab and the published data. Further, statistical testing showed the differences are significant.

1. Average insert size: Because the published dataset is single end, this metric is no longer applicable, as all inserts are the same length in single-end data and can’t be compared to paired-end. 
2. [Duplicate reads rate:](https://github.com/TatyanaLev/EE282/blob/finalProjectReport/output/figures/dup.png) Duplicate reads rate was significantly lower in our lab data (p < 2.2e-16). This means that the published dataset was over-sequenced, and lab data was less saturated in this regard. Lab duplicate rate is better.
3. [Average Phred quality score:](https://github.com/TatyanaLev/EE282/blob/finalProjectReport/output/figures/qual.png) lab sequences were significantly better than the published ones (p = 5.745e-12), probably because sequencing reagents improved in the 5 years between the published data and lab experiment. Exome kits were different as well.
4. [Sequence depth:](https://github.com/TatyanaLev/EE282/blob/finalProjectReport/output/figures/depth.png) lab data was significantly lower depth (p = 4.92e-15). This means that the exome was sequenced less in our library than in the published one. This has implications for variant calling. Somatic mutations can be missed this way. Our data is worse in this regard.
5. [5X Sequence breadth:](https://github.com/TatyanaLev/EE282/blob/finalProjectReport/output/figures/breadth.png) lab data was significantly lower breadth than the published data (p < 2.2e-16). This means that a smaller percentage of the whole exome was covered by at least 5 reads. Cells with large gaps in coverage do not pass QC, as loci with no information cannot be used for clustering. Our data is worse.

DISCUSSION

The quality metrics comparison results are mixed, but it is clear that our lab data differs significantly from the published dataset. In the most important quality metrics, sequence depth and breadth, our lab data is significantly worse. To improve, I would need to troubleshoot the library preparation and load more sample for sequencing. Even though my project analysis showed that there is a data quality problem, it does not rule out additional problems in my downstream analysis. Next, I will do variant calling on the published data and cluster it to try to reproduce figure 2A in Zafar et al. 
 
CODE

[scripts](https://github.com/TatyanaLev/EE282/tree/finalProjectReport/code/scripts)
 
REFERENCES

Wang, Y., Waters, J., Leung, M. et al. Clonal evolution in breast cancer revealed by single nucleus genome sequencing. Nature 512, 155–160 (2014). https://doi.org/10.1038/nature13600

Zafar, H., Wang, Y., Nakhleh, L. et al. Monovar: single-nucleotide variant detection in single cells. Nat Methods 13, 505–507 (2016). https://doi.org/10.1038/nmeth.3835 
